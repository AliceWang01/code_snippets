{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bandata.api import get\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month value to use when calculating months since September\n",
    "sept = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Querying the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Redshift for the JD Power Valuations\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(\n",
    "    host='xxxx',\n",
    "    database='xxx',\n",
    "    port=xxx,\n",
    "    user='xx',\n",
    "    password='xx'\n",
    ")\n",
    "\n",
    "# Query a table using the Cursor\n",
    "sql = \"\"\"\n",
    "WITH all_data AS\n",
    "(\n",
    "    SELECT\n",
    "        mv.bam_period as period,\n",
    "        mv.bam_region AS region_id,\n",
    "        r.region_name,\n",
    "        mv.bam_ucgvehicleid as ucgvehicleid,\n",
    "        vl.YEAR,\n",
    "        vl.make,\n",
    "        vl.model,\n",
    "        vl.bodystyle,\n",
    "        mv.basecleantrade,\n",
    "        mv.basecleanretail,\n",
    "        mv.averagemileage,\n",
    "        ma.lowlevel,\n",
    "        ma.highlevel,\n",
    "        ma.adjamount,\n",
    "        mv.baseaverageetrade + ma.adjamount AS adjustedcleantrade,\n",
    "        mv.basecleanretail + ma.adjamount AS adjustedcleanretail\n",
    "    FROM bigdataprod.jdpower.v_monthly_valuations_latest_by_date AS mv\n",
    "    JOIN bigdataprod.jdpower.v_vehicle_list_latest_by_date AS vl\n",
    "    ON mv.bam_ucgvehicleid = vl.ucgvehicleid\n",
    "    JOIN jdpower.regions AS r\n",
    "    ON mv.bam_region = r.region_id\n",
    "    JOIN jdpower.v_mileage_adjustments_latest_by_date AS ma\n",
    "    ON mv.bam_ucgvehicleid = ma.bam_ucgvehicleid\n",
    "    AND mv.bam_period = ma.bam_period\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    ucgvehicleid, -- The JD Power Vehicle ID that identifies a Make/Model/Year/Trim\n",
    "    period, -- The period that the valuation is for\n",
    "    region_id, -- Region ID that the valuation is for\n",
    "    region_name, -- Name of the region\n",
    "    make, -- Make of vehicle\n",
    "    model, -- Model of vehicle\n",
    "    YEAR, -- Vehicle model year\n",
    "    bodystyle, -- Essentially the trim level\n",
    "    basecleantrade, -- Trade in value given the average mileage for this model year in this period\n",
    "    basecleanretail, -- Retail value given the average mileage for this model year in this period\n",
    "    averagemileage, -- Average mileage for this model and year in the selected period\n",
    "    lowlevel, -- Low mileage for this model and year in the selected period\n",
    "    highlevel, -- High mileage for this model and year in the selected period\n",
    "    adjamount, -- Adjustment amount for an instance of this ucgvehicleid\n",
    "    adjustedcleantrade, -- Clean trade value for a 30K version of this ucgvehicleid with 30,000 miles\n",
    "    adjustedcleanretail -- Clean retail value for a 30K version of this ucgvehicleid\n",
    "FROM all_data\n",
    "WHERE\n",
    "    (30000 BETWEEN lowlevel AND highlevel) -- update mileage anchor as desired\n",
    "    AND basecleanretail <> 0\n",
    "ORDER BY\n",
    "    ucgvehicleid,\n",
    "    period,\n",
    "    region_id\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment\n",
    "dfResults = pd.read_sql(sql, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets work on a copy of the data so we don't need to requery if we want to start again.\n",
    "dfResultsWithName = dfResults.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull actual CPI data for autos\n",
    "# Convert the period to a date\n",
    "dfResultsWithName['period'] = pd.to_datetime(dfResultsWithName['period']).dt.date\n",
    "\n",
    "# get the min and max dates out of the valuation results data\n",
    "mindate = datetime.datetime.combine(dfResultsWithName['period'].min(), datetime.datetime.min.time())\n",
    "maxdate = datetime.datetime.combine(dfResultsWithName['period'].max(), datetime.datetime.min.time())\n",
    "\n",
    "# get CPI Data for this range\n",
    "# CPRTUCTR Index - US CPI Urban Consumers Used Cars & Trucks NSA\n",
    "# CPSTUCTR Index - US CPI Urban Consumers Used Cars & Trucks SA\n",
    "\n",
    "dfCPI = get(\n",
    "    \"BloombergApi.BloombergHistoricalData\",\n",
    "    Auth=dict(Type='App', Name='BAM:SDA'),\n",
    "    api_parameters=dict(data_environment='PROD'),  # PROD for production\n",
    "    Symbols=dict(Symbolology='Ticker', IDs=('CPRTUCTR Index')),\n",
    "    Fields=('PX_OPEN', 'PX_LAST', 'PX_VOLUME'),\n",
    "    Periodicity='MONTHLY',\n",
    "    StartDate=pd.Timestamp(mindate),\n",
    "    EndDate=pd.Timestamp(maxdate)\n",
    ")\n",
    "\n",
    "# make sure period columns is a date\n",
    "dfCPI['period'] = pd.to_datetime(dfCPI['TIMESTAMP']).dt.date\n",
    "\n",
    "# cosmetic change to column name\n",
    "dfCPI.rename(columns={'PX_LAST': 'CPRTUCTR'}, inplace=True)\n",
    "dfCPI.drop(['SYMBOL', 'TIMESTAMP'], axis=1, inplace=True)\n",
    "\n",
    "# Calculate month over month change in inflation\n",
    "dfCPI['pctChangeInflation'] = dfCPI['CPRTUCTR'].pct_change()\n",
    "\n",
    "dfCPI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge CPI data into valuation data\n",
    "dfResultsWithName = pd.merge(dfResultsWithName, dfCPI, how='left', on='period')\n",
    "dfResultsWithName.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis dataframe with a subset of the columns\n",
    "dfAnalysis = dfResultsWithName[['ucgvehicleid', 'region_id', 'region_name', 'period', 'basecleanretail', 'averagemileage', \n",
    "                                'adjustedcleanretail', 'year', 'make', 'model', 'CPRTUCTR', 'pctChangeInflation']].copy()\n",
    "\n",
    "# make sure retail number is numeric\n",
    "dfAnalysis['adjustedcleanretail'] = pd.to_numeric(dfAnalysis['adjustedcleanretail'])\n",
    "\n",
    "# filter just for testing. todo: remove\n",
    "dfAnalysis = dfAnalysis[(dfAnalysis['ucgvehicleid']==20150805) & (dfAnalysis['region_id']==1)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate month over month change in price\n",
    "dfAnalysis = dfAnalysis.sort_values(['ucgvehicleid', 'region_id', 'period'])\n",
    "dfAnalysis['PctChangeAdjustedCleanRetail'] = dfAnalysis.groupby(['ucgvehicleid', 'region_id'])['adjustedcleanretail'].pct_change()\n",
    "dfAnalysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datetime column so that we can use the dt.month function\n",
    "dfAnalysis['periodDT'] = pd.to_datetime(dfAnalysis['period'], errors='coerce', utc=True)\n",
    "\n",
    "# Calculate months since September\n",
    "dfAnalysis['months_since_september'] = (dfAnalysis['periodDT'].dt.month - sept) % 12\n",
    "\n",
    "# Drop the date time column we don't need anymore\n",
    "dfAnalysis.drop(['periodDT'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repetition of above steps using local csv (for personal use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to local csv\n",
    "jdp = pd.read_csv('jdp.csv')  # dfResultsWithName from above saved locally as a csv file\n",
    "jdp['period'] = pd.to_datetime(jdp['period'])\n",
    "\n",
    "# Read CPI data from a local csv file\n",
    "dfCPI = pd.read_csv('cpi.csv')\n",
    "dfCPI['period'] = pd.to_datetime(dfCPI['period'])\n",
    "\n",
    "# Merge CPI data into valuation data\n",
    "jdp = pd.merge(jdp, dfCPI, how='left', on='period')\n",
    "\n",
    "# Create analysis dataframe with a subset of the columns\n",
    "dfAnalysis = jdp[['ucgvehicleid', 'region_id', 'region_name', 'period', 'basecleanretail', \n",
    "                  'averagemileage', 'adjustedcleanretail', 'year', 'make', 'model', \n",
    "                  'CPRTUCTR', 'PctChangeInflation']].copy()\n",
    "\n",
    "# Make sure retail number is numeric\n",
    "dfAnalysis['adjustedcleanretail'] = pd.to_numeric(dfAnalysis['adjustedcleanretail'])\n",
    "\n",
    "# Calculate month over month change in price\n",
    "dfAnalysis = dfAnalysis.sort_values(['ucgvehicleid', 'region_id', 'period'])\n",
    "dfAnalysis['PctChangeAdjustedCleanRetail'] = dfAnalysis.groupby(['ucgvehicleid', 'region_id'])['adjustedcleanretail'].pct_change()\n",
    "\n",
    "# Create a datetime column so that we can use the dt.month function\n",
    "dfAnalysis['periodDT'] = pd.to_datetime(dfAnalysis['period'], errors='coerce', utc=True)\n",
    "\n",
    "# Calculate months since September\n",
    "dfAnalysis['months_since_september'] = (dfAnalysis['periodDT'].dt.month - sept) % 12\n",
    "\n",
    "# Drop the date time column we don't need anymore\n",
    "dfAnalysis.drop(['periodDT'], axis=1, inplace=True)\n",
    "\n",
    "# Convert 'period' back to datetime after dropping 'periodDT'\n",
    "dfAnalysis['period'] = pd.to_datetime(dfAnalysis['period'])\n",
    "\n",
    "# Display the first few rows of the analysis dataframe\n",
    "dfAnalysis.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Manipulating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating depreciation adjusted prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new df named \"DAP\" for Depreciation Adjusted Price\n",
    "dap = dfAnalysis.copy()\n",
    "dap = dap.sort_values(['ucgvehicleid', 'region_id', 'period'])\n",
    "\n",
    "# Get adjusted clean retail price for next vintage (v+1)\n",
    "vp1 = dap[['ucgvehicleid', 'period', 'make', 'model', 'year', 'region_id', 'adjustedcleanretail']].copy()\n",
    "vp1['year'] = vp1['year'] - 1\n",
    "vp1 = vp1.rename(columns={'ucgvehicleid': 'ucgvehicleid_v', 'adjustedcleanretail': 'adjustedcleanretail_vp1'})\n",
    "\n",
    "# Merge to calculate depreciation adjusted price\n",
    "dap = pd.merge(dap, vp1, on=['period', 'make', 'model', 'year', 'region_id'])\n",
    "dap['depreciation_adj_price'] = dap['adjustedcleanretail'] * (dap['adjustedcleanretail_vp1'] / dap['adjustedcleanretail'] + 1) ** (dap['months_since_september'] / 12)\n",
    "\n",
    "# Display the first few rows of the DAP dataframe\n",
    "dap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percent changes in adjusted clean retail (ACR) prices and depreciation adjusted prices (DAP)\n",
    "dap['pctDAP'] = dap.groupby(['ucgvehicleid', 'region_id'])['depreciation_adj_price'].pct_change()\n",
    "dap['pctACR'] = dap.groupby(['ucgvehicleid', 'region_id'])['adjustedcleanretail'].pct_change()\n",
    "\n",
    "# Get average values for each period\n",
    "dap['avg_pctDAP'] = dap.groupby('period')['pctDAP'].transform('mean')\n",
    "dap['avg_pctACR'] = dap.groupby('period')['pctACR'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding \"smoothing\" to avoid September drops without model changeover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months_since_september=12 in September rather than 0\n",
    "dap['months_since_september2'] = dap['months_since_september'].replace(0, 12)\n",
    "\n",
    "# quality adjustment set to 1\n",
    "quality_adjustment = 1\n",
    "\n",
    "# depreciation adjusted price calculation using altered months_since_september\n",
    "dap['depreciation_adj_price2'] = dap['adjustedcleanretail'] * ((dap['adjustedcleanretail_vp1'] / dap['adjustedcleanretail'] + 1) ** (dap['months_since_september2'] / 12))\n",
    "\n",
    "# Percent change in depreciation adjusted prices using altered months_since_september\n",
    "dap['pctDAP2'] = dap.groupby(['ucgvehicleid', 'region_id'])['depreciation_adj_price2'].pct_change()\n",
    "\n",
    "# Get average values for depreciation adjusted prices percent change\n",
    "dap['avg_pctDAP2'] = dap.groupby('period')['pctDAP2'].transform('mean')\n",
    "dap['avg_pctDAP_smooth'] = dap['avg_pctDAP']\n",
    "\n",
    "# Replace percent change in September with new value; avoids issue where months_since_september changes from 11 to 0\n",
    "dap.loc[dap['period'].dt.month == sept, 'avg_pctDAP_smooth'] = dap['avg_pctDAP2']\n",
    "\n",
    "# Display the first few rows to verify changes\n",
    "dap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Changeover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vintage year associated with a particular period and specified age value\n",
    "def get_vintage(date, age) -> int:\n",
    "    date = pd.to_datetime(date)\n",
    "    if date.month < sept:  # Assuming 'sept' is previously defined as 9\n",
    "        # Get older vintage if month is before September\n",
    "        return date.year - (age + 1)\n",
    "    else:\n",
    "        # Get 1 year newer vintage otherwise\n",
    "        return date.year - age\n",
    "   \n",
    "for i in range(2, 7):\n",
    "    # Create new dataframe for each age value 2-6\n",
    "    dfVi = dap.copy()\n",
    "    # Limit dataframe to vehicles of the specified age\n",
    "    dfVi = dfVi[dfVi['year'] == dfVi['period'].apply(lambda x: get_vintage(x, i))]\n",
    "    # Get pct change in DAP grouped by make, model, region ID, and age; we don't group by ID since ID/year changes every September\n",
    "    dfVi['pctDAP_changeover'] = dfVi.groupby(['make', 'model', 'region_id'])['depreciation_adj_price'].pct_change()\n",
    "    \n",
    "    dfVi['age'] = i\n",
    "    if i == 2:\n",
    "        dfV = dfVi\n",
    "    else:\n",
    "        dfV = pd.concat([dfV, dfVi])\n",
    "\n",
    "col_names = []\n",
    "# Get average pct DAP for each individual age value 2-6\n",
    "for i in range(2, 7):\n",
    "    dfV['avg_pctDAP_changeover' + str(i) + 'yr'] = dfV.groupby('period')['pctDAP_changeover'].transform(lambda x: x[dfV['age'] == i].mean())\n",
    "    col_names.append('avg_pctDAP_changeover' + str(i) + 'yr')\n",
    "\n",
    "# Get average pct DAP across all age values 2-6\n",
    "dfV['avg_pctDAP_changeover'] = dfV.groupby('period')['pctDAP_changeover'].transform('mean')\n",
    "\n",
    "dfV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate columns of interest from model changeover dataframe (dfV)\n",
    "dfV_columns = ['period', 'ucgvehicleid', 'region_id', 'pctDAP_changeover', 'avg_pctDAP_changeover'] + col_names\n",
    "\n",
    "# Add model changeover calculations to original dataframe\n",
    "dap = pd.merge(dap, dfV[dfV_columns], on=['period', 'ucgvehicleid', 'region_id'], how='left')  # Left merge such that all dates remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date selected such that all columns are populated\n",
    "dap[dap['period'] == '2021-11-30'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Regional Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from census import Census\n",
    "from us import states\n",
    "\n",
    "# Uncomment as needed\n",
    "\n",
    "# Get corresponding J.D. Power region ID for each state. Does not decrement license.\n",
    "def get_request(endpoint:str, arguments:str, prod=False, echoRequestString = False):\n",
    "    # API keys - pull from vault eventually\n",
    "    prodAPIKey = \"d8fa95b9-987b-4538-aa96-b5ce684bf2c2\"\n",
    "    TestAPIKey = \"4d7cc8ce-1cbc-49ce-a918-840a861a1825\"\n",
    "\n",
    "    # URLs - Pull from consul eventually. Dev URL affects license count and has the same data as prod, but will have new features and end points a few weeks before prod url\n",
    "    prodURL = \"https://cloud.jdpower.ai/data-api/valuationservices/valuation/\"\n",
    "    devURL = \"https://cloud.jdpower.ai/data-api/UAT/valuationservices/valuation/\"\n",
    "\n",
    "    # Create empty dataframe for results\n",
    "    dfResult = pd.DataFrame()\n",
    "    # Set API key and URL based on whether we want prod or test\n",
    "    if prod == True:\n",
    "        requestString = prodURL + endpoint + arguments\n",
    "        apiKey = prodAPIKey\n",
    "    else:\n",
    "        requestString = devURL + endpoint + arguments\n",
    "        apiKey = TestAPIKey\n",
    "\n",
    "    # The request string is nice to have when talking to JD Power support\n",
    "    if echoRequestString == True:\n",
    "        print(requestString)\n",
    "\n",
    "    # Make the request\n",
    "    r = requests.get(requestString, headers={\"api-key\": apiKey})\n",
    "    if r.status_code == 200:\n",
    "        if echoRequestString == True:\n",
    "            print('Success: ' + requestString)\n",
    "\n",
    "        # Store resulting json\n",
    "        json = r.json()\n",
    "\n",
    "        # Convert results to a dataframe from json['result']\n",
    "        dfResult = pd.DataFrame(json['result'])\n",
    "\n",
    "    # else clause for failed HTTP request\n",
    "    else:\n",
    "        print('Failure')\n",
    "        print(r)\n",
    "\n",
    "    # Return result dataframe. Will be empty if the request failed\n",
    "    return dfResult\n",
    "\n",
    "# Variables for API endpoint and parameters\n",
    "endpoint = 'regionByIDStateCode'\n",
    "period = '2022-09-30'\n",
    "userinfo = 'xxxx'\n",
    "\n",
    "# Codes represent different U.S. states\n",
    "codes = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', \n",
    "            'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'DC']\n",
    "\n",
    "# Initialize an empty DataFrame for collecting regions\n",
    "dfRegions = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in range(len(codes)):\n",
    "    arguments = '?statecode=' + codes[i] + '&userinfo=' + userinfo\n",
    "    dfState = get_request(endpoint=endpoint, arguments=arguments, prod=True, echoRequestString=False)\n",
    "    dfState.insert(0, 'state', codes[i])\n",
    "    dfRegions = pd.concat([dfRegions, dfState])\n",
    "\n",
    "# Change to personal Census API Key\n",
    "c = Census(\"xxxx\")\n",
    "\n",
    "# Define list of states\n",
    "state_list = [state.name for state in states.STATES]\n",
    "\n",
    "# Retrieve population data for each state\n",
    "populations = {}\n",
    "for state in state_list:\n",
    "    # Fetch population data using Census API, querying for state FIPS codes\n",
    "    population_data = c.sf1.get((\"NAME\", 'P001001'), {'for': 'state:{}'.format(states.lookup(state).fips)})\n",
    "    populations[state] = population_data[0]['P001001']\n",
    "\n",
    "# Add District of Columbia\n",
    "population_data = c.sf1.get((\"NAME\", 'P001001'), {'for': 'state:{}'.format('11')})\n",
    "populations['District of Columbia'] = population_data[0]['P001001']\n",
    "\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\"\n",
    "}\n",
    "\n",
    "# Maps state abbreviations to full state names\n",
    "us_abbrev_to_state = {v: k for k, v in us_state_to_abbrev.items()}\n",
    "\n",
    "# Create a copy of the dfRegions dataframe\n",
    "dfPop = dfRegions.copy()\n",
    "\n",
    "# Get 2010 state population given state abbreviation\n",
    "def get_pop(abbrev):\n",
    "    state = us_abbrev_to_state[abbrev]\n",
    "    return populations[state]\n",
    "\n",
    "# Add population data to dfPop\n",
    "dfPop['pop'] = dfPop['state'].transform(lambda x: get_pop(x))\n",
    "dfPop['pop'] = dfPop['pop'].astype(int)\n",
    "\n",
    "# Get total population for each region\n",
    "dfRegional = dfPop[['regionid', 'pop']].groupby('regionid').sum()\n",
    "\n",
    "# Calculate weights as a percentage of total population\n",
    "dfRegional['pop'] = dfRegional['pop'] / dfRegional['pop'].sum()\n",
    "dfRegional = dfRegional.sort_values(by='pop', ascending=False)\n",
    "dfRegional = dfRegional.reset_index()\n",
    "\n",
    "dfRegional = dfRegional.rename(columns={'regionid': 'region_id'})\n",
    "dfRegional['region_id'] = dfRegional['region_id'].astype(int)\n",
    "dap['region_id'] = dap['region_id'].astype(int)\n",
    "dap = pd.merge(dap, dfRegional, on='region_id')\n",
    "dap = dap.rename(columns={'pop': 'region_weight'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For personal use - storing regional weights so I don't need to rerun the previous cell\n",
    "dfRegional.to_csv('region_weights.csv', index=False)\n",
    "\n",
    "# For personal use - reading regional weights from locally saved CSV\n",
    "dfRegional = pd.read_csv('region_weights.csv')\n",
    "dap['region_id'] = dap['region_id'].astype(int)\n",
    "dap = pd.merge(dap, dfRegional, on='region_id')\n",
    "dap = dap.rename(columns={'pop': 'region_weight'})\n",
    "\n",
    "# Display the DataFrame\n",
    "dfRegional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value we want to average using regional weights\n",
    "value = 'pctDAP'\n",
    "\n",
    "# Take the average of the specified value within each region for each period\n",
    "by_region = dap.groupby(['region_id', 'period']).agg({value: 'mean', 'region_weight': 'first'})\n",
    "by_region = by_region.reset_index()\n",
    "\n",
    "# Use regional weights to compute a weighted average for each period\n",
    "weighted_avg = by_region.groupby('period').apply(\n",
    "    lambda x: (x[value] * x['region_weight']).sum()\n",
    ")\n",
    "weighted_avg = pd.DataFrame(weighted_avg)\n",
    "weighted_avg = weighted_avg.rename(columns={0: 'avg_' + value + '_weighted'})\n",
    "\n",
    "# Add weighted average to main df\n",
    "dap = dap.merge(weighted_avg, on='period')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "# Define the estimates and titles\n",
    "estimates = ['avg_pctACR', 'avg_pctDAP', 'avg_pctDAP_smooth', 'avg_pctDAP_changeover']  # 'avg_pctDAP_weighted'\n",
    "titles = ['ACR Price', 'DAP', 'Smoothed DAP', 'DAP with Model Changeover']\n",
    "\n",
    "# Loop through each estimate and create a subplot\n",
    "for i in range(len(estimates)):\n",
    "    estimate = estimates[i]\n",
    "    title = titles[i]\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Add vertical lines for September 1st of each year\n",
    "    # Comment to omit\n",
    "    for year in range(2016, 2024):  # May need to alter dates according to estimation variable\n",
    "        ax.axvline(dt.datetime(year, 9, 30), color='c', linestyle='--')\n",
    "\n",
    "    dfGraph = dap[['period', 'pctChangeInflation', estimate]]\n",
    "    dfGraph = dfGraph.dropna().drop_duplicates()\n",
    "    dfGraph.plot(x='period', y=['pctChangeInflation', estimate], ax=ax)\n",
    "\n",
    "    legend = plt.legend()\n",
    "    legend.get_texts()[0].set_text('True Change')\n",
    "    legend.get_texts()[1].set_text('Predicted Change')\n",
    "    plt.title('Monthly UTC Predictions Using '+ title)  # Change title as appropriate\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('MoM Percent Change')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Numerica Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation coefficients, R-squared values, and hit rates evaluating each variable as an estimate of inflation (UCT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = ['avg_pctACR', 'avg_pctDAP', 'avg_pctDAP_smooth', 'avg_pctDAP_changeover', 'avg_pctDAP_weighted']\n",
    "\n",
    "for estimate in estimates:\n",
    "    dfClean = dap[['period', 'pctChangeInflation', estimate]]\n",
    "    dfClean = dfClean.dropna().drop_duplicates()\n",
    "    # Limit timeframe to 2018 onward due to sample size\n",
    "    dfClean = dfClean[dfClean['period'] >= '2018-01-01']\n",
    "\n",
    "    corr_coef = dfClean['pctChangeInflation'].corr(dfClean[estimate])\n",
    "    r_squared = corr_coef ** 2\n",
    "    hit_rate = ((dfClean['pctChangeInflation'] * dfClean[estimate]) >= 0).mean()\n",
    "    print('Estimate: ' + estimate + ', Corr Coef: ' + str(corr_coef) + ', R-squared: ' + str(r_squared) + ', Hit Rate: ' + str(hit_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate: avg_pctACR\n",
    "  Corr Coef: 0.85458410631952\n",
    "  R-squared: 0.730270077178805\n",
    "  Hit Rate: 0.7878787878787878\n",
    "\n",
    "Estimate: avg_pctDAP\n",
    "  Corr Coef: 0.9340578611863622\n",
    "  R-squared: 0.872463715497617\n",
    "  Hit Rate: 0.9090909090909091\n",
    "\n",
    "Estimate: avg_pctDAP_smooth\n",
    "  Corr Coef: 0.8653702544413996\n",
    "  R-squared: 0.7495799593855096\n",
    "  Hit Rate: 0.8787878787878788\n",
    "\n",
    "Estimate: avg_pctDAP_changeover\n",
    "  Corr Coef: 0.8555447720240712\n",
    "  R-squared: 0.731965873073135\n",
    "  Hit Rate: 0.06060606060606061\n",
    "\n",
    "Estimate: avg_pctDAP_weighted\n",
    "  Corr Coef: 0.934053712851616\n",
    "  R-squared: 0.872463384918891\n",
    "  Hit Rate: 0.9090909090909091\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Correlation coefficients by age value')\n",
    "\n",
    "corr_coef = dap['pctChangeInflation'].corr(dap['avg_pctDAP_changeover'])\n",
    "r_squared = corr_coef ** 2\n",
    "print('All values 2-6:', corr_coef, r_squared)\n",
    "\n",
    "for i in range(2, 7):\n",
    "    corr_coef = dap['pctChangeInflation'].corr(dap['avg_pctDAP_changeover' + str(i) + 'yr'])\n",
    "    r_squared = corr_coef ** 2\n",
    "    print('Age value = ' + str(i) + ':', corr_coef, r_squared)\n",
    "\n",
    "# Correlation over recent years\n",
    "dfByAge = dap.copy()\n",
    "dfByAge = dfByAge[dfByAge['period'] >= '2021-0{}-01'.format(sept)]\n",
    "print('Correlation coefficients since Sep 2021')\n",
    "\n",
    "corr_coef = dfByAge['pctChangeInflation'].corr(dfByAge['avg_pctDAP_changeover'])\n",
    "r_squared = corr_coef ** 2\n",
    "print('All values 2-6:', corr_coef, r_squared)\n",
    "\n",
    "for i in range(2, 7):\n",
    "    corr_coef = dfByAge['pctChangeInflation'].corr(dfByAge['avg_pctDAP_changeover' + str(i) + 'yr'])\n",
    "    r_squared = corr_coef ** 2\n",
    "    print('Age value = ' + str(i) + ':', corr_coef, r_squared)\n",
    "\n",
    "dfByAge['avg_pctDAP_changeover34'] = (dfByAge['avg_pctDAP_changeover3yr'] + dfByAge['avg_pctDAP_changeover4yr']) / 2\n",
    "corr_coef = dfByAge['pctChangeInflation'].corr(dfByAge['avg_pctDAP_changeover34'])\n",
    "r_squared = corr_coef ** 2\n",
    "print('Age values 3 & 4:', corr_coef, r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification reports and conclusion matrices for each estimate of inflation (UCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "estimates = ['avg_pctACR', 'avg_pctDAP', 'avg_pctDAP_smooth', 'avg_pctDAP_changeover', 'avg_pctDAP_weighted']\n",
    "\n",
    "for estimate in estimates:\n",
    "    dfClean = dap[['period', 'pctChangeInflation', estimate]]\n",
    "    dfClean = dfClean.dropna().drop_duplicates()\n",
    "    \n",
    "    # Limit timeframe to 2018 onward due to sample size\n",
    "    dfClean = dfClean[dfClean['period'] >= '2018-01-01']\n",
    "    \n",
    "    # Generate a confusion matrix and classification report for the current estimate\n",
    "    y_true = dfClean['pctChangeInflation'] >= 0\n",
    "    y_pred = dfClean[estimate] >= 0\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cr = classification_report(y_true, y_pred)\n",
    "    \n",
    "    # Print the results for the current estimate\n",
    "    print(f'Results for {estimate}:')\n",
    "    print('Classification Report:')\n",
    "    print(cr)\n",
    "    \n",
    "    # Create heatmap of confusion matrix\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix: ' + estimate)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes in the sample through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique vehicle ID's present in the sample over time\n",
    "region_sample_counts = dap.groupby(['region_id', 'period'])['ucgvehicleid'].nunique().reset_index()\n",
    "region_sample_counts.plot(x='period', y='ucgvehicleid', ylabel='vehicle count', title=\"Number of Unique Vehicle ID's by Period\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS Regression to combine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "cols = ['avg_pctACR', 'avg_pctDAP', 'avg_pctDAP_smooth', 'avg_pctDAP_changeover']\n",
    "\n",
    "dfModel = dap[['period', 'pctChangeInflation'] + cols]\n",
    "dfModel = dfModel[dfModel['period'] >= '2018-01-01']\n",
    "dfModel = dfModel.dropna()\n",
    "dfModel = dfModel.drop_duplicates()\n",
    "\n",
    "dfModel[cols] = dfModel[cols].apply(normalize)\n",
    "\n",
    "x = dfModel[cols]\n",
    "y = dfModel['pctChangeInflation']\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "model = sm.OLS(y, x).fit(cov_type='HC3')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)\n",
    "dfGraph = dfModel\n",
    "\n",
    "dfGraph['predicted_values'] = y_pred\n",
    "dfGraph['true_values'] = y\n",
    "\n",
    "dfGraph.plot(x='period', y=['predicted_values', 'true_values'], xlabel='Date', ylabel='MoM Percent Change', title='Monthly UCT Predictions Using OLS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coef = dfGraph['true_values'].corr(dfGraph['predicted_values'])\n",
    "r_squared = corr_coef ** 2\n",
    "hit_rate = ((dfGraph['true_values'] >= 0) == (dfGraph['predicted_values'] >= 0)).mean()\n",
    "\n",
    "print('Corr Coef: ' + str(corr_coef) + ', R-squared: ' + str(r_squared) + ', Hit Rate: ' + str(hit_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix and classification report for OLS\n",
    "y_true_bool = y >= 0\n",
    "y_pred_bool = y_pred >= 0\n",
    "cm = confusion_matrix(y_true_bool, y_pred_bool)\n",
    "cr = classification_report(y_true_bool, y_pred_bool)\n",
    "\n",
    "# Print the results for the current estimate\n",
    "print('Results for OLS:')\n",
    "print('Classification Report:')\n",
    "print(cr)\n",
    "\n",
    "# Create heatmap of confusion matrix\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix: OLS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### July estimate of the UCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['avg_pctACR', 'avg_pctDAP', 'avg_pctDAP_smooth', 'avg_pctDAP_changeover']\n",
    "\n",
    "dfModel = dap[['period', 'pctChangeInflation'] + cols]\n",
    "dfModel = dfModel[dfModel['period'] >= '2018-01-01']\n",
    "dfModel = dfModel.dropna()\n",
    "dfModel = dfModel.drop_duplicates()\n",
    "\n",
    "x = dfModel[cols]\n",
    "y = dfModel['pctChangeInflation']\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "model = sm.OLS(y, x).fit(cov_type='HC3')\n",
    "\n",
    "x_last_row = dap[['period'] + cols].dropna().tail(1)\n",
    "print(x_last_row)\n",
    "x_last_row = x_last_row.drop('period', axis=1)\n",
    "x_last_row.insert(0, 'const', 1)\n",
    "x_last_row = x_last_row.reset_index(drop=True)\n",
    "\n",
    "estimate = model.predict(x_last_row)\n",
    "print(f'OLS Estimate: {estimate.iloc[0]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot percent change in ACR by region, will see little diff which explains why regional weighting makes little difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['pctACR', 'adjustedcleanretail']\n",
    "for value in values:\n",
    "    region_data = dap.groupby(['region_id', 'period'])[value].mean()\n",
    "    region_data = pd.DataFrame(region_data)\n",
    "    region_data.reset_index(inplace=True)\n",
    "    region_data['period'] = pd.to_datetime(region_data['period'])\n",
    "    region_data = region_data.pivot(index='period', columns='region_id', values=value)\n",
    "    \n",
    "    \n",
    "    region_data.plot(ylabel='average percent change in vehicle price', title=value + ' by region')\n",
    "\n",
    "    region_data = region_data.reset_index()\n",
    "    # Changes are clearer over a more limited window\n",
    "    region_data = region_data[region_data['period'] >= '2018-01-01']\n",
    "    region_data = region_data[region_data['period'] < '2019-01-01']\n",
    "    \n",
    "    ax = region_data.plot(x='period', xlabel='period', title=value + ' by region (2018)')\n",
    "    \n",
    "    # Add the horizontal line at y=0, useful for graphing % change\n",
    "    # ax.axhline(y=0, color='gray', linestyle='--')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
